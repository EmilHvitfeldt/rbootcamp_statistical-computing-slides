---
title: 'R Bootcamp day 3<br>Statistical Computing Track<br><img src="../fig/trojan-rlogo.svg" alt="trojan R logo" style="width:250px;">'
author: <a href="https://ggvy.cl"><b>George G. Vega Yon</b></a><br>vegayon@usc.edu<br><br>University of Southern California<br>Department of Preventive Medicine
date: August 21th, 2019
output: 
  slidy_presentation:
    footer: R Bootcamp (Day 3)
    font_adjustment: -1
    incremental: true
---

# Agenda {style="width: 80%;margin: auto;height: 80%;"}

1.  High-Performance Computing: An overview
    
2.  Parallel computing in R
    
3.  Extended examples

You can download the <a href="parallel.Rmd" target="_blank">Rmd file here</a> and
the R code <a href="parallel.R" target="_blank">Rmd file here</a>

# High-Performance Computing: An overview {style="width: 80%;margin: auto;height: 80%;"}

Loosely, from R's perspective, we can think of HPC in terms of two, maybe three things:

1.  Big data: How to work with data that doesn't fit your computer

2.  Parallel computing: How to take advantage of multiple core systems

3.  Compiled code: Write your own low-level code (if R doesn't has it yet...)

(Checkout [CRAN Task View on HPC](https://cran.r-project.org/web/views/HighPerformanceComputing.html))


# Big Data {style="width: 80%;margin: auto;height: 80%;"}

*   Buy a bigger computer/RAM memory (not the best solution!)
    
*   Use out-of-memory storage, i.e., don't load all your data in the RAM. e.g.
    The [bigmemory](https://CRAN.R-project.org/package=bigmemory),
    [data.table](https://CRAN.R-project.org/package=data.table),
    [HadoopStreaming](https://CRAN.R-project.org/package=HadoopStreaming) R packages
    
*   Efficient algorithms for big data, e.g.: [biglm](https://cran.r-project.org/package=biglm),
    [biglasso](https://cran.r-project.org/package=biglasso)

*   Store it more efficiently, e.g.: Sparse Matrices (take a look at the `dgCMatrix` objects
    from the [Matrix](https://CRAN.R-project.org/package=Matrix) R package)

# Parallel computing {style="width: 80%;margin: auto;height: 80%;"}

```{r, echo=FALSE, fig.cap="Flynn's Classical Taxonomy ([Introduction to Parallel Computing, Blaise Barney, Lawrence Livermore National Laboratory](https://computing.llnl.gov/tutorials/parallel_comp/#Whatis))", fig.align='center'}
knitr::include_graphics("../fig/flynnsTaxonomy.gif")
```

We will be focusing on the **S**ingle **I**nstruction stream **M**ultiple **D**ata stream

# Some vocabulary for HPC {style="width: 80%;margin: auto;height: 80%;"}

In raw terms

*   Supercomputer: A **single** big machine with thousands of cores/gpus.

*   High Performance Computing (HPC): **Multiple** machines within
    a **single** network.
    
*   High Throughput Computing (HTC): **Multiple** machines across **multiple**
    networks.
    
You may not have access to a supercomputer, but certainly HPC/HTC clusters are
more accesible these days, e.g. AWS provides a service to create HPC clusters
at a low cost (allegedly, since nobody understands how pricing works)

# What's "a core"? {style="width: 80%;margin: auto;height: 80%;text-align:center"}

![Taxonomy of CPUs (Downloaded from de https://slurm.schedmd.com/mc_support.html)](../fig/cpu-slurm.png){width="400px"}

Now, how many cores does your computer has, the parallel package can tell you that:

```{r 03-how-many-cores}
parallel::detectCores()
```

# What is parallel computing, anyway? {style="width: 80%;margin: auto;height: 80%;"}

```r
f <- function(n) n*2
f(1:4)
```

![Here we are using a single core. The function is applied one element at a time, leaving the other 3 cores without usage.](../fig/pll-computing-explained-serial.svg){width="35%"}

# What is parallel computing, anyway? (cont'd) {style="width: 80%;margin: auto;height: 80%;"}

```r
f <- function(n) n*2
f(1:4)
```

![In this more intelligent way of computation, we are taking full advantage of our computer by using all 4 cores at the same time. This will translate in a reduced computation time which, in the case of complicated/long calculations, can be an important speed gain.](../fig/pll-computing-explained-parallel.svg){width="35%"}


# GPU vs CPU {style="width: 80%;margin: auto;height: 80%;"}

```{r gpu-cpu, echo=FALSE, fig.cap="[NVIDIA Blog](http://www.nvidia.com/object/what-is-gpu-computing.html)", fig.align='center', out.width="400px"}
knitr::include_graphics("../fig/cpuvsgpu.jpg")
nnodes <- 4L
```

> * Why use OpenMP if GPU is _suited to compute-intensive operations_? Well, mostly because
    OpenMP is **VERY** easy to implement (easier than CUDA, which is the easiest way to use GPU).



# {style="background-color:#515A5A;margin:auto;text-align:center;"}

<text style="color:white;">Let's think before we start...</text>

![](https://media.giphy.com/media/Dwclsfe6Gb91m/giphy.gif){style="width:500px"}

<text style="color:white;">When is it a good idea to go HPC?</text>

# When is it a good idea? {style="width: 80%;margin: auto;height: 80%;"}

```{r good-idea, echo=FALSE, fig.cap="Ask yourself these questions before jumping into HPC!", fig.align='center', out.width="60%"}
knitr::include_graphics("../fig/when_to_parallel.svg")
```

# Parallel computing in R {style="width: 80%;margin: auto;height: 80%;"}

While there are several alternatives (just take a look at the
[High-Performance Computing Task View](https://cran.r-project.org/web/views/HighPerformanceComputing.html)),
we'll focus on the following R-packages for **explicit parallelism**:

*   [**parallel**](https://cran.r-project.org/package=parallel): R package that provides '[s]upport for parallel computation,
    including random-number generation'.

*   [**foreach**](https://cran.r-project.org/package=foreach): R package for 'general iteration over elements' in parallel fashion.

*   [**future**](https://cran.r-project.org/package=future): '[A] lightweight and
    unified Future API for sequential and parallel processing of R
    expression via futures.' (won't cover here)
    
Implicit parallelism, on the other hand, are out-of-the-box tools that allow the
programmer not to worry about parallelization, e.g. such as
[**gpuR**](https://cran.r-project.org/package=gpuR) for Matrix manipulation using
GPU, [**tensorflow**](https://cran.r-project.org/package=tensorflow)

# {style="width: 80%;margin: auto;height: 80%;"}

And there's also a more advanced set of options

*   [**Rcpp**](https://cran.r-project.org/package=Rcpp) + [OpenMP](https://www.openmp.org):
    [Rcpp](https://cran.r-project.org/package=Rcpp) is an R package for integrating
    R with C++, and OpenMP is a library for high-level parallelism for C/C++ and
    Fortran.

*   A ton of other type of resources, notably the tools for working with 
    batch schedulers such as Slurm, HTCondor, etc.

# Parallel workflow {style="width: 80%;margin: auto;height: 80%;"}

(Usually) We do the following:

1.  Create a `PSOCK/FORK` (or other) cluster using `makePSOCKCluster`/`makeForkCluster`
    (or `makeCluster`)
    
2.  Copy/prepare each R session (if you are using a `PSOCK` cluster):

    a.  Copy objects with `clusterExport`

    b.  Pass expressions with `clusterEvalQ`

    c.  Set a seed

3.  Do your call: `parApply`, `parLapply`, etc. 

4.  Stop the cluster with `clusterStop`


# Types of clusters: PSOCK {style="width: 80%;margin: auto;height: 80%;"}

-   Can be created with `makePSOCKCluster`

-   Creates brand new R Sessions (so nothing is inherited from the master), e.g.
    
    ```r
    # This creates a cluster with 4 R sessions
    cl <- makePSOCKCluster(4)
    ```

-   Child sessions are connected to the master session via Socket connections

-   Can be created outside of the current computer, i.e. accross multiple computers!

# Types of clusters: Fork {style="width: 80%;margin: auto;height: 80%;"}

-   Fork Cluster `makeForkCluster`:

-   Uses OS [Forking](https://en.wikipedia.org/wiki/Fork_(system_call)),

-   Copies the current R session locally (so everything is inherited from
    the master up to that point).
    
-   Data is only duplicated if it is altered (need to double check when this happens!)

-   Not available on Windows.

-   Other `makeCluster`: passed to [**snow**](https://cran.r-project.org/package=snow)
    (Simple Network of Workstations)

Take a look at the [foreach](https://cran.r-project.org/package=foreach),
[Rmpi](https://cran.r-project.org/package=Rmpi), and
[future](https://cran.r-project.org/package=future) R packages.

# {style="background-color:#515A5A;margin:auto;text-align:center"}

<text style="color:white;">Let's get started!</text>

# Ex 1: Parallel RNG with `makePSOCKCluster` {style="width: 80%;margin: auto;height: 80%;"}


```{r parallel-ex-psock, echo=TRUE}
# 1. CREATING A CLUSTER
library(parallel)
nnodes <- 4L
cl     <- makePSOCKcluster(nnodes)    

# 2. PREPARING THE CLUSTER
clusterSetRNGStream(cl, 123) # Equivalent to `set.seed(123)`

# 3. DO YOUR CALL
ans <- parSapply(cl, 1:nnodes, function(x) runif(1e3))
(ans0 <- var(ans))
```

# {style="width: 80%;margin: auto;height: 80%;"}

Making sure is reproducible

```{r parallel-ex-psock-cont, echo=TRUE}
# I want to get the same!
clusterSetRNGStream(cl, 123)
ans1 <- var(parSapply(cl, 1:nnodes, function(x) runif(1e3)))

# 4. STOP THE CLUSTER
stopCluster(cl)

all.equal(ans0, ans1) # All equal!
```

# Ex 2: Parallel RNG with `makeForkCluster` {style="width: 80%;margin: auto;height: 80%;"}

In the case of `makeForkCluster`

```{r parallel-ex-fork, echo=TRUE, eval = TRUE}
# 1. CREATING A CLUSTER
library(parallel)

# The fork cluster will copy the -nsims- object
nsims  <- 1e3
nnodes <- 4L
cl     <- makeForkCluster(nnodes)    

# 2. PREPARING THE CLUSTER
clusterSetRNGStream(cl, 123)

# 3. DO YOUR CALL
ans <- do.call(cbind, parLapply(cl, 1:nnodes, function(x) {
  runif(nsims) # Look! we use the nsims object!
               # This would have fail in makePSOCKCluster
               # if we didn't copy -nsims- first.
  }))

(ans0 <- var(ans))
```

# {style="width: 80%;margin: auto;height: 80%;"}

Again, we want to make sure this is reproducible

```{r parallel-ex-fork-cont, echo=TRUE}
# Same sequence with same seed
clusterSetRNGStream(cl, 123)
ans1 <- var(do.call(cbind, parLapply(cl, 1:nnodes, function(x) runif(nsims))))

ans0 - ans1 # A matrix of zeros

# 4. STOP THE CLUSTER
stopCluster(cl)
```

# {style="background-color:#515A5A;margin:auto; text-align:center"}

```{r what-did-you-said, out.width="60%", echo=FALSE}
knitr::include_graphics("../fig/what-did-you-said.gif")
```

<text style="color:white;">Well, if you are a MacOS/Linux user, there's a simplier way of doing this...</text>


# Ex 3: Parallel RNG with `mclapply` (Forking on the fly) {style="width: 80%;margin: auto;height: 80%;"}

In the case of `mclapply`, the forking (cluster creation) is done on the fly!

```{r parallel-ex-mclapply, echo=TRUE, eval = TRUE}
# 1. CREATING A CLUSTER
library(parallel)

# The fork cluster will copy the -nsims- object
nsims  <- 1e3
nnodes <- 4L
# cl     <- makeForkCluster(nnodes) # mclapply does it on the fly

# 2. PREPARING THE CLUSTER
set.seed(123) 

# 3. DO YOUR CALL
ans <- do.call(cbind, mclapply(1:nnodes, function(x) runif(nsims)))

(ans0 <- var(ans))
```

# {style="width: 80%;margin: auto;height: 80%;"}

Once more, we want to make sure this is reproducible

```{r parallel-ex-mclapply-cont, echo=TRUE}
# Same sequence with same seed
set.seed(123) 
ans1 <- var(do.call(cbind, mclapply(1:nnodes, function(x) runif(nsims))))

ans0 - ans1 # A matrix of zeros

# 4. STOP THE CLUSTER
# stopCluster(cl) no need of doing this anymore
```

# Ex 4: RcppArmadillo + OpenMP {style="width: 80%;margin: auto;height: 80%;"}

```{Rcpp dist-code, cache=TRUE, echo=TRUE}
#include <omp.h>
#include <RcppArmadillo.h>

// [[Rcpp::depends(RcppArmadillo)]]
// [[Rcpp::plugins(openmp)]]

using namespace Rcpp;

// [[Rcpp::export]]
arma::mat dist_par(const arma::mat & X, int cores = 1) {
  
  // Some constants
  int N = (int) X.n_rows;
  int K = (int) X.n_cols;
  
  // Output
  arma::mat D(N,N);
  D.zeros(); // Filling with zeros
  
  // Setting the cores
  omp_set_num_threads(cores);
  
#pragma omp parallel for shared(D, N, K, X) default(none)
  for (int i=0; i<N; ++i)
    for (int j=0; j<i; ++j) {
      for (int k=0; k<K; k++) 
        D.at(i,j) += pow(X.at(i,k) - X.at(j,k), 2.0);
      
      // Computing square root
      D.at(i,j) = sqrt(D.at(i,j));
      D.at(j,i) = D.at(i,j);
    }
      
  
  // My nice distance matrix
  return D;
}
```

# {style="width: 80%;margin: auto;height: 80%;"}

```{r dist-dat, dependson=-1, echo=TRUE, cache=TRUE}
# Simulating data
set.seed(1231)
K <- 1000
n <- 500
x <- matrix(rnorm(n*K), ncol=K)

# Are we getting the same?
table(as.matrix(dist(x)) - dist_par(x, 4)) # Only zeros
```

# {style="width: 80%;margin: auto;height: 80%;"}

```{r dist-benchmark, echo=TRUE, cache=TRUE}
# Benchmarking!
rbenchmark::benchmark(
  dist(x),                 # stats::dist
  dist_par(x, cores = 1),  # 1 core
  dist_par(x, cores = 2),  # 2 cores
  dist_par(x, cores = 4), #  4 cores
  replications = 10, order="elapsed"
)[,1:4]
```

# Ex 5: The future {style="width: 80%;margin: auto;height: 80%;"}

```{r future, echo=TRUE, collapse=TRUE, cache=TRUE}
library(future)
plan(multicore)

# We are creating a global variable
a <- 2

# Creating the futures has only the overhead (setup) time
system.time({
  x1 %<-% {Sys.sleep(3);a^2}
  x2 %<-% {Sys.sleep(3);a^3}
})

# Let's just wait 5 seconds to make sure all the cores have returned
Sys.sleep(3)
system.time({
  print(x1)
  print(x2)
})
```


# Ex 6: Simulating $\pi$ {style="width: 80%;margin: auto;height: 80%;"}


*   We know that $\pi = \frac{A}{r^2}$. We approximate it by randomly adding
    points $x$ to a square of size 2 centered at the origin.

*   So, we approximate $\pi$ as $\Pr\{\|x\| \leq 1\}\times 2^2$

```{r, echo=FALSE, dev='jpeg', dev.args=list(quality=100), fig.width=6, fig.height=6, out.width='300px', out.height='300px'}
set.seed(1231)
p    <- matrix(runif(5e3*2, -1, 1), ncol=2)
pcol <- ifelse(sqrt(rowSums(p^2)) <= 1, adjustcolor("blue", .7), adjustcolor("gray", .7))
plot(p, col=pcol, pch=18)
```

# {style="width: 80%;margin: auto;height: 80%;"}

The R code to do this

```{r simpi, echo=TRUE}
pisim <- function(i, nsim) {  # Notice we don't use the -i-
  # Random points
  ans  <- matrix(runif(nsim*2), ncol=2)
  
  # Distance to the origin
  ans  <- sqrt(rowSums(ans^2))
  
  # Estimated pi
  (sum(ans <= 1)*4)/nsim
}
```

# Ex 6: Simulating $\pi$ (cont'd) {style="width: 80%;margin: auto;height: 80%;"}

```{r parallel-ex2, echo=TRUE, cache=TRUE}

# Setup
cl <- makePSOCKcluster(4L)
clusterSetRNGStream(cl, 123)

# Number of simulations we want each time to run
nsim <- 1e5

# We need to make -nsim- and -pisim- available to the
# cluster
clusterExport(cl, c("nsim", "pisim"))

# Benchmarking: parSapply and sapply will run this simulation
# a hundred times each, so at the end we have 1e5*100 points
# to approximate pi
rbenchmark::benchmark(
  parallel = parSapply(cl, 1:100, pisim, nsim=nsim),
  serial   = sapply(1:100, pisim, nsim=nsim), replications = 1
)[,1:4]

```

# {style="width: 80%;margin: auto;height: 80%;"}

```{r printing-and-stop, cache=TRUE}
ans_par <- parSapply(cl, 1:100, pisim, nsim=nsim)
ans_ser <- sapply(1:100, pisim, nsim=nsim)
stopCluster(cl)
```

```{r, echo=FALSE}
c(par = mean(ans_par), ser = mean(ans_ser), R = pi)
```


# {style="text-align:center!important;"}

```{r thanks, out.width="300px", echo=FALSE}
knitr::include_graphics("../fig/speed.gif")
```

## Thanks!

<p style="text-align:center!important;">
`r icon::fa("github")`  [gvegayon](https://github.com/gvegayon/) <br>
`r icon::fa("twitter")`  [\@gvegayon](https://twitter.com/gvegayon) <br>
`r icon::fa("home")`  [ggvy.cl](https://ggvy.cl)<br><br>
<text style="color:gray;font-size:80%">Presentation created with [rmarkdown::slidy_presentation](https:cran.r-project.org/package=rmarkdown)</text>
</p>

# See also {style="width: 80%;margin: auto;height: 80%;"}

*   [Package parallel](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf) 
*   [Using the iterators package](https://cran.r-project.org/web/packages/iterators/vignettes/iterators.pdf)
*   [Using the foreach package](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.pdf)
*   [32 OpenMP traps for C++ developers](https://software.intel.com/en-us/articles/32-openmp-traps-for-c-developers)
*   [The OpenMP API specification for parallel programming](http://www.openmp.org/)
*   ['openmp' tag in Rcpp gallery](gallery.rcpp.org/tags/openmp/)
*   [OpenMP tutorials and articles](http://www.openmp.org/resources/tutorials-articles/)

For more, checkout the [CRAN Task View on HPC](https://cran.r-project.org/web/views/HighPerformanceComputing.html){target="_blank"}

# Session info

```{r session, echo=FALSE}
sessionInfo()
```

